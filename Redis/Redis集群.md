# Redis集群

## 1.redis高可用集群架构

![image-20211204205426026](C:/Users/wangzhao/AppData/Roaming/Typora/typora-user-images/image-20211204205426026.png)

Redis集群是一个由多个主从节点群组成的分布式服务器群，它具有复制、高可用和分片特性。redis集群不需要sentinel哨兵，也能完成节点转移和故障转移的功能。需要将每个节点设置成集群模式，这种模式中没有中心点，可水平扩展，官方文档中提到可以扩展到10000多节点(但是官方推荐不要超过1000)，redis集群的性能和高可用均优于之前版本的哨兵模式，且集群配置非常简单。

## 2.redis集群原理分析

redis cluster将所有的数据的划分成16384个slots，每个节点负责一部分槽位。槽位的信息存储在每个节点中。当redis cluster的客户端连接到集群，客户端会得到一份集群的槽位数据，并缓存到客户端。当客户端获取某个key的时候，直接在客户端槽位缓存数据中直接直接定位到对应节点。同时因为槽位的信息可能会存在客户端和服务器集群信息不一致的情况，还需要通过纠正机制来实现槽位信息的校验调整。

### 2.1槽定位算法

cluster默认会对key值使用crc16算法进行hash得到一个整数值，然后用这个值和16384进行取模来得到具体槽位。

HASH_SLOT = CRC16(KEY) mod 16384

### 2.2跳转重定位

当一个客户端向一个错误的节点发送了指令，该节点发现指令的key的槽位并不在本节点中，这时候回向客户端发送一个特殊的跳跃指针携带目标操作的节点地址，告诉客户端去连这个节点并获取数据。客户端收到指令后除了跳转到正确的节点上操作，同时还会同步更新本地的槽位缓存映射表，后面所有的key都会使用新的槽位映射表。

### 2.3redis节点间的通信机制

**redis cluster节点间采用gossip协议进行通信**

维护集群元数据有两种方式：集中式和gossip

**集中式：**

优点在于源数据的更新和读取，时效性好，一旦源数据出现变更就会更新到集中式的存储中，其他节点读取的时候就会立即感知到；缺点是所有源数据的更新都集中在一个地方，可能导致源数据的存储压力。很多中间件会借助zookeeper集中式存储源数据。

**gossip**

gossip协议包括多种信息，包括ping、meet、pong、fail

**meet**：某个节点发送meet给新加入的节点，新节点加入到集群中，然后新节点会开始和其他节点进行通讯

**ping**：每个节点都会频繁的给其他节点发送ping信息，包含自己的状态还有维护集群源数据，互相通过ping交换元数据(类似感知到新增或删除节点)

**pong**：作为ping信息的回应，包含自身状态和其他信息，也可以用于信息的广播和更新

**fail**：某个节点判断另一个节点为fail之后，就发送fail给其他节点，通知其他节点，指定的节点宕机了

gossip协议的优点在于对源数据的更新比较分散，不是集中在一个地方，更新请求会陆陆续续达到其他节点进行更新，有一定的延迟，但是分担了压力；缺点在于源数据的更新缓慢可能会导致集群的后续操作受到影响。

### 2.4网络抖动

网络出现抖动是常见的一种现象，某节点突然之间出现不可访问，然后又很快恢复正常。

为了解决这个问题，redis cluster提供一种选项cluster-node-timeout，表示当某个节点持续timeout的时间后，才会认定这个节点出现故障，需要进行主从切换。如果没有这个选项，网络波动会造成持续的主从切换，意味着数据的复制。

### 2.5redis集群选举原理分析

当slave发现自己的master处于fail状态后，便尝试进行failover，尝试成为新的master。由于挂掉的master可能会有多个slave，从而会有多个slave竞争成为新的master，过程如下：

1. slave发现自己master的状态为fail
2. 将自己记录集群的currentepoch加1，并广播failover_auth_request信息
3. 其他节点收到此条信息，只有master进行回应，并发送failover_auth_ack，对每个epoch只发送一次ack
4. 尝试failover的slave收集master返回的failover_auth_ack
5. slave收到超过半数的master发送到ack后变成新的master
6. slave广播pong消息通知其他节点

从节点并不是在主节点进入到fail状态以后马上发起选举，而是有一定的延迟，这个延迟确保我们等待fail的状态在集群中传播，slave如果立即尝试选举，其他master尚未意识到fail状态，可能拒绝投票

延迟计算公式：

DELAY = 500ms + random(0-500ms) + slave_rank * 1000ms

这里的slave_rank代表此slave已经从master复制的数据的总量的rank，rank越小代表复制的数据越新。在这种方式下，理论上复制数据更多的slave会先发送消息。

### 2.6集群脑裂数据丢失问题

![image-20211204213255177](C:/Users/wangzhao/AppData/Roaming/Typora/typora-user-images/image-20211204213255177.png)

redis中没有过半机制就会出现脑裂问题，网络分区导致脑裂后多个主节点对外提供写服务，通俗的讲就是一个小的集群中出现了两个master对外提供服务，一旦网络分区恢复，会将其中一个节点变为从节点，这时候就会出现大量数据丢失的问题。

规避的方法需要在redis的配置添加参数`min‐replicas‐to‐write 1 `，这个1代表的是写数据成功最少同步的slave的数量，这个数量可以模仿大于半数机制配置，比如集群中总共有三个节点可以配置成1，加上leader就会有2，超过半数。

图中的master由于某些原因，这个小集群发生了选举换了master，这时候之前的master恢复，产生了两个master，由于在配置文件中配置了`min‐replicas‐to‐write 1`，因此之前的master在统计时的节点数量并没有超过半数，因此就会变成slave。

注意：这个配置在一定程度上会影响集群的可用性，比如slave要是少于1，这个集群就算是leader处于正常状态，也不能提供服务。

### 2.7redis节点数量分析

redis的mater节点数量至少为三。因为新master的选举需要超过半数(master数量/2)集群的master进行投票才能选举成功，如果只有两个master的话，当其中一个挂掉，只剩下一个master，没有超过半数，因此是不能进行投票选举的。

为什么要推荐使用奇数个master节点，三个master集群和四个master集群进行选举时，当有一个master节点挂掉以后，都能进行新的选举，因为剩下的master超过了半数，但是当挂掉两个master节点就不会进行选举，因为剩下的节点没有超过半数。其实选择奇数个master节点主要是从**节省机器资源**的角度进行考虑的。